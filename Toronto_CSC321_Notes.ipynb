{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "So, it's linear, right? We're talking `y = m*x + b` shit. \n",
    "Well, check out [this badassery](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec02.pdf).\n",
    "\n",
    "Your data approximates some trend described by some line. That line is described by a function.\n",
    "Your neural network is composed of weights (w) and biases (b). Your weights and biases combined with your input parameter (x) describes some trend line of outputs (y). Therefore, our weights and biases describe some ideal trend line, `y = w * x + b`. \n",
    "\n",
    "Now, that trend line is not gonna be perfect. It's an ideal, after all. The data will differ. To account for those differences and pursue a more accurate trend line, we need some loss function. The loss function should be minimized, i.e. as close to 0 as possible. The loss function can be used to calculate our \"residuals\". A residual as the difference betwene our trend line and the real data point at a given x. Using our loss function to modify our weights/biases, we can reduce our residuals.\n",
    "\n",
    "So what is this loss function? Based on my previous learning, there are many options. For the above lecture and linear regression, we use a \"squared error\" function:\n",
    "> L(y, t) = 1/2 (y - t)^2\n",
    "\n",
    "Verbally, that's the loss function of y wrt t is one have of the square of y minus t. \n",
    "\n",
    "In addition to a \"loss function\", we have a \"cost function\". The difference is a loss function represents a snapshot in time, i.e. a single data set/point, and a cost function is across the whole set/time. Given that `y = w * x + b` and we're calculating this for a number of inputs, we can substitute and expand over inputs as...\n",
    "> E(w, b) = 1/2N * sigma(i=1 => N)(y(i-th) - t(i-th))^2\n",
    "> E(w, b) = 1/2N * sigma(i=1 => N)(w*x(i-th) + b - t(i-th))^2\n",
    "\n",
    "For now though, let's show what our prediction / trend line would look like with weights/biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x: 4 --- according to our prediction, output y: 5\n",
      "input x: 5 --- according to our prediction, output y: 15\n",
      "input x: 6 --- according to our prediction, output y: 33\n"
     ]
    }
   ],
   "source": [
    "b = 1 # some bias\n",
    "M = 3 # the size of our data set?\n",
    "\n",
    "w = [1, 2, 3] # our weights\n",
    "x = [4, 5, 6] # our x values / inputs\n",
    "\n",
    "y = b # b is like our y-intercept\n",
    "for j in range(M):\n",
    "    y += w[j] * x[j]\n",
    "    print('input x: ' + str(x[j]) + ' --- according to our prediction, output y: ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is cool, but for sufficiently large data, it'll be slow. Enter vectors and matrices and GPUs. Gotta go fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x values: [4 5 6]\n",
      "final y value : 33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([1, 2, 3])\n",
    "x = np.array([4, 5, 6])\n",
    "y = np.dot(w, x) + b\n",
    "print('input x values: ' + str(x))\n",
    "print('final y value : ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's.... so fresh, so clean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
