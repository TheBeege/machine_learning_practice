{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "So, it's linear, right? We're talking `y = m*x + b` shit. \n",
    "Well, check out [this badassery](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec02.pdf).\n",
    "\n",
    "Your data approximates some trend described by some line. That line is described by a function.\n",
    "Your neural network is composed of weights (w) and biases (b). Your weights and biases combined with your input parameter (x) describes some trend line of outputs (y). Therefore, our weights and biases describe some ideal trend line, `y = w * x + b`. \n",
    "\n",
    "Now, that trend line is not gonna be perfect. It's an ideal, after all. The data will differ. To account for those differences and pursue a more accurate trend line, we need some loss function. The loss function should be minimized, i.e. as close to 0 as possible. The loss function can be used to calculate our \"residuals\". A residual as the difference betwene our trend line and the real data point at a given x. Using our loss function to modify our weights/biases, we can reduce our residuals.\n",
    "\n",
    "So what is this loss function? Based on my previous learning, there are many options. For the above lecture and linear regression, we use a \"squared error\" function:\n",
    "> L(y, t) = 1/2 (y - t)^2\n",
    "\n",
    "Verbally, that's the loss function of y wrt t is one have of the square of y minus t. \n",
    "\n",
    "In addition to a \"loss function\", we have a \"cost function\". The difference is a loss function represents a snapshot in time, i.e. a single data set/point, and a cost function is across the whole set/time. Given that `y = w * x + b` and we're calculating this for a number of inputs, we can substitute and expand over inputs as...\n",
    "> E(w, b) = 1/2N * sigma(i=1 => N)(y(i-th) - t(i-th))^2\n",
    "> E(w, b) = 1/2N * sigma(i=1 => N)(w*x(i-th) + b - t(i-th))^2\n",
    "\n",
    "For now though, let's show what our prediction / trend line would look like with weights/biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x: 4 --- according to our prediction, output y: 5\n",
      "input x: 5 --- according to our prediction, output y: 15\n",
      "input x: 6 --- according to our prediction, output y: 33\n"
     ]
    }
   ],
   "source": [
    "b = 1 # some bias\n",
    "M = 3 # the size of our data set?\n",
    "\n",
    "w = [1, 2, 3] # our weights\n",
    "x = [4, 5, 6] # our x values / inputs\n",
    "\n",
    "y = b # b is like our y-intercept\n",
    "for j in range(M):\n",
    "    y += w[j] * x[j]\n",
    "    print('input x: ' + str(x[j]) + ' --- according to our prediction, output y: ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is cool, but for sufficiently large data, it'll be slow. Enter vectors and matrices and GPUs. Gotta go fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x values: [4 5 6]\n",
      "final y value : 33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([1, 2, 3])\n",
    "x = np.array([4, 5, 6])\n",
    "y = np.dot(w, x) + b\n",
    "print('input x values: ' + str(x))\n",
    "print('final y value : ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's.... so fresh, so clean.\n",
    "\n",
    "BUT WAIT, I'M BILLY MAYS, AND THERE'S MORE! (RIP Billy Mays)\n",
    "\n",
    "Let's say we have a training data set. That is, we have several sets of X and Y pairs that we can use to train our model. That's also easier to do with matrices in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [8, 0, 3, 0],\n",
    "    [6, -1, 5, 3],\n",
    "    [2, 5, -2, 8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each column as a \"feature\". Thtat is, a feature of a point in time for the larger pattern (I think. This is how I'm interpreting it. I could be wrong.)\n",
    "Each row is a training example - a vector of real-world training data.\n",
    "\n",
    "Now for our weights.\n",
    "\n",
    "Also, my linear algebra is shit. I don't know how I passed that class in college. Dot product works like this:\n",
    "> For each entry of a row, you multiple by the corresponding entry of the other matrix's column\n",
    "\n",
    "I often refer to [this page](https://www.mathsisfun.com/algebra/matrix-multiplying.html) to remind myself how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should be able to calculate the Mth y for each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Mth (length of X's rows) y for each \"training\" example: [18 32 39]\n"
     ]
    }
   ],
   "source": [
    "y = np.dot(X, w) + b\n",
    "print('Our Mth (length of X\\'s rows) y for each \"training\" example: ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each of our 3 training data vectors, we have the Mth y. Pretty schwifty.\n",
    "\n",
    "But that's for prediction of our weights and biases. What about our loss function? Let's code that up.\n",
    "Quick reminder of our cost function:\n",
    "> E(w, b) = 1/2N * sigma(i=1 => N)(w*x(i-th) + b - t(i-th))^2\n",
    "\n",
    "Ah, before that, we gotta define some stuff. In this, \"t\" is our target y, that is, the y from our real training data. N is the number of t values, the number of y values from our training data.\n",
    "\n",
    "From the online lectures, they're using some training data set they found online. I don't necessarily want to copy it exactly, so... I'm just gonna make shit up and hope for the best! Really scientific over here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "t = np.array([16, 32, 41])\n",
    "N = len(t)\n",
    "cost = np.sum((y - t) ** 2) / (2. * N)\n",
    "print('cost: ' + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, I just took our training y values and added and subtracted from them a bit. So our cost function produces a pretty low ... cost? Still learning vocabulary here. Regardless, we want to minimize this. We want to get this as close to 0 as possible. If there's a smooth function, that is, some function that perfectly predicts our data (I think?), the minimum of this function is where the derivative = 0. This is called the \"critical point\", apparently. I don't recall this from my calculus classes. I blame sleep deprivation. \n",
    "In the case of multiple variables, that is, \"multivariate generalization\", if we get the partial derivatives to 0, we get the \"direct solution\". I don't understand what exactly what this means yet, but we'll get there... I hope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
